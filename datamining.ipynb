{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1KfwvJGZCuMewyy_WZ3Rp0--sRy1nXzYS","authorship_tag":"ABX9TyMvlWEzG9wRmYH1eHnx8hCM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import pickle\n","import os\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.svm import SVC  # Using SVM instead of Naive Bayes\n","from sklearn.metrics import classification_report, accuracy_score\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","\n","# File paths\n","training_file = '/content/drive/MyDrive/Data_Mining/twitter_training.csv'\n","validation_file = '/content/drive/MyDrive/Data_Mining/twitter_validation.csv'\n","\n","# Dataset columns and encoding\n","DATASET_COLUMNS = ['id', 'category', 'sentiment', 'text']\n","DATASET_ENCODING = \"ISO-8859-1\"\n","\n","# Load datasets\n","train_df = pd.read_csv(training_file, encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n","valid_df = pd.read_csv(validation_file, encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n","\n","# Map sentiment labels to numerical values\n","sentiment_mapping = {\n","    \"Negative\": 0,\n","    \"Neutral\": 2,\n","    \"Positive\": 4,\n","    \"Irrelevant\": 1\n","}\n","\n","train_df['sentiment'] = train_df['sentiment'].map(sentiment_mapping)\n","valid_df['sentiment'] = valid_df['sentiment'].map(sentiment_mapping)\n","\n","# Drop unnecessary columns (keeping only 'text' and 'sentiment')\n","train_df = train_df[['text', 'sentiment']]\n","valid_df = valid_df[['text', 'sentiment']]\n","\n","# Verify mapping\n","print(\"Training Data Target Values:\", train_df['sentiment'].unique())\n","print(\"Validation Data Target Values:\", valid_df['sentiment'].unique())\n","\n","# Fill missing values with empty strings\n","train_df['text'] = train_df['text'].fillna(\"\").astype(str)\n","valid_df['text'] = valid_df['text'].fillna(\"\").astype(str)\n","\n","# Initialize NLTK tools\n","stop_words = set(stopwords.words('english'))\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stemmer = PorterStemmer()\n","\n","# Text preprocessing function\n","def preprocess_text(text):\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    tokens = tokenizer.tokenize(text.lower())  # Tokenize and lowercase\n","    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n","    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  # Stemming\n","    return ' '.join(stemmed_tokens)\n","\n","# Apply preprocessing to text data\n","train_df['text'] = train_df['text'].apply(preprocess_text)\n","valid_df['text'] = valid_df['text'].apply(preprocess_text)\n","\n","# TF-IDF Vectorization\n","vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n","X_train_tfidf = vectorizer.fit_transform(train_df['text'])\n","X_valid_tfidf = vectorizer.transform(valid_df['text'])\n","\n","# Feature selection using Chi-Square\n","chi2_selector = SelectKBest(chi2, k=3000)\n","X_train_chi2 = chi2_selector.fit_transform(X_train_tfidf, train_df['sentiment'])\n","X_valid_chi2 = chi2_selector.transform(X_valid_tfidf)\n","\n","# Train SVM model\n","svm_model = SVC(kernel='linear')  # You can change the kernel (e.g., 'rbf', 'poly')\n","svm_model.fit(X_train_chi2, train_df['sentiment'])\n","\n","# Define the directory where you want to save the files\n","save_directory = '/content/drive/MyDrive/Data_Mining/myModel'\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(save_directory, exist_ok=True)\n","\n","# Save the Model, Vectorizer, and Chi-Square Selector\n","with open(os.path.join(save_directory, 'svm_model.pkl'), 'wb') as model_file:\n","    pickle.dump(svm_model, model_file)\n","\n","with open(os.path.join(save_directory, 'vectorizer.pkl'), 'wb') as vec_file:\n","    pickle.dump(vectorizer, vec_file)\n","\n","with open(os.path.join(save_directory, 'chi2_selector.pkl'), 'wb') as chi2_file:\n","    pickle.dump(chi2_selector, chi2_file)\n","\n","print(f\"Model and associated files saved in: {save_directory}\")\n","\n","# Evaluate the model\n","y_pred = svm_model.predict(X_valid_chi2)\n","print(\"Classification Report:\")\n","print(classification_report(valid_df['sentiment'], y_pred))\n","print(\"Accuracy:\", accuracy_score(valid_df['sentiment'], y_pred))\n","\n","# Function to predict sentiment for new text\n","def predict_sentiment(text):\n","    # Load the saved model, vectorizer, and chi-square selector\n","    with open(os.path.join(save_directory, 'svm_model.pkl'), 'rb') as model_file:\n","        svm_model = pickle.load(model_file)\n","\n","    with open(os.path.join(save_directory, 'vectorizer.pkl'), 'rb') as vec_file:\n","        vectorizer = pickle.load(vec_file)\n","\n","    with open(os.path.join(save_directory, 'chi2_selector.pkl'), 'rb') as chi2_file:\n","        chi2_selector = pickle.load(chi2_file)\n","\n","    # Preprocess the input text\n","    processed_text = preprocess_text(text)\n","    features = vectorizer.transform([processed_text])\n","    features = chi2_selector.transform(features)\n","\n","    # Predict sentiment\n","    prediction = svm_model.predict(features)\n","\n","    # Map numerical prediction back to sentiment label\n","    sentiment_mapping_reverse = {0: \"Negative\", 1: \"Irrelevant\", 2: \"Neutral\", 4: \"Positive\"}\n","    return sentiment_mapping_reverse[prediction[0]]\n","\n","# Test the prediction function\n","sample_tweet = \"I love this product! Highly recommended.\"\n","# sample_tweet = \"I hate this product\"\n","print(\"Sample Tweet Prediction:\", predict_sentiment(sample_tweet))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIoHgsDLIrHD","executionInfo":{"status":"ok","timestamp":1737369845507,"user_tz":-60,"elapsed":618559,"user":{"displayName":"ossama boufarra","userId":"12857159768935969023"}},"outputId":"86747121-c32e-4663-892e-7fef9a8041de"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Training Data Target Values: [4 2 0 1]\n","Validation Data Target Values: [1 2 0 4]\n","Model and associated files saved in: /content/drive/MyDrive/Data_Mining/myModel\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.88      0.81       266\n","           1       0.80      0.72      0.75       172\n","           2       0.83      0.71      0.76       285\n","           4       0.77      0.81      0.79       277\n","\n","    accuracy                           0.78      1000\n","   macro avg       0.79      0.78      0.78      1000\n","weighted avg       0.79      0.78      0.78      1000\n","\n","Accuracy: 0.783\n","Sample Tweet Prediction: Positive\n"]}]},{"cell_type":"code","source":["import pickle\n","import os\n","import re\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","import nltk\n","\n","# Download NLTK resources (if not already downloaded)\n","nltk.download('stopwords')\n","\n","# Define the directory where the model and associated files are saved\n","save_directory = '/content/drive/MyDrive/Data_Mining/myModel'\n","\n","# Text preprocessing function (must match the one used during training)\n","def preprocess_text(text):\n","    stop_words = set(stopwords.words('english'))\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    stemmer = PorterStemmer()\n","\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    tokens = tokenizer.tokenize(text.lower())  # Tokenize and lowercase\n","    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n","    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  # Stemming\n","    return ' '.join(stemmed_tokens)\n","\n","# Function to predict sentiment for new text\n","def predict_sentiment(text):\n","    # Load the saved model, vectorizer, and chi-square selector\n","    with open(os.path.join(save_directory, 'svm_model.pkl'), 'rb') as model_file:\n","        svm_model = pickle.load(model_file)\n","\n","    with open(os.path.join(save_directory, 'vectorizer.pkl'), 'rb') as vec_file:\n","        vectorizer = pickle.load(vec_file)\n","\n","    with open(os.path.join(save_directory, 'chi2_selector.pkl'), 'rb') as chi2_file:\n","        chi2_selector = pickle.load(chi2_file)\n","\n","    # Preprocess the input text\n","    processed_text = preprocess_text(text)\n","    features = vectorizer.transform([processed_text])\n","    features = chi2_selector.transform(features)\n","\n","    # Predict sentiment\n","    prediction = svm_model.predict(features)\n","\n","    # Map numerical prediction back to sentiment label\n","    sentiment_mapping_reverse = {0: \"Negative\", 1: \"Irrelevant\", 2: \"Neutral\", 4: \"Positive\"}\n","    return sentiment_mapping_reverse[prediction[0]]\n","\n","# Test the prediction function with a new sample tweet\n","sample_tweet = \"This movie was fantastic! I really enjoyed it.\"\n","# sample_tweet = \"I had a terrible experience with this product.\"\n","print(\"Sample Tweet Prediction:\", predict_sentiment(sample_tweet))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zm4SyEZOQf7o","executionInfo":{"status":"ok","timestamp":1737369180400,"user_tz":-60,"elapsed":274,"user":{"displayName":"ossama boufarra","userId":"12857159768935969023"}},"outputId":"9b2593c9-ba1b-43fb-a5f6-d5291be86f28"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Tweet Prediction: Positive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]}]}